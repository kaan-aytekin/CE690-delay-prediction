{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from warnings import filterwarnings\n",
    "from pprint import pprint\n",
    "import gc\n",
    "import dill\n",
    "filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, BayesianRidge, ARDRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import make_scorer,get_scorer, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tune_sklearn import TuneSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "# Reference: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIRECTORY = \"/home/kaan.aytekin/Thesis\"\n",
    "# Non-feature columns\n",
    "non_feature_columns = [\"simulation_run\", \"is_accident_simulation\", \n",
    "                       \"accident_location\", \"accident_start_time\", \n",
    "                       \"accident_duration\", \"accident_lane\", \n",
    "                       \"prev_detector_detector_number\",\"next_detector_detector_number\",\n",
    "                       \"detector_number\", \"timestamp\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_array(array,freq):\n",
    "    array_size = len(array)\n",
    "    sample_size = int(np.ceil(array_size*freq))\n",
    "    array_slicer = np.zeros(array_size)\n",
    "    test_index =  np.random.choice(range(0,array_size),size=sample_size,replace=False)\n",
    "    array_slicer[test_index] = 1\n",
    "    return array[array_slicer.astype(bool)]\n",
    "\n",
    "def kfolds_from_array(array,k,seed=None):\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    np.random.shuffle(array)\n",
    "    array_folds = np.array_split(array,k)\n",
    "    return array_folds\n",
    "\n",
    "def simulation_based_k_folds_split(df,k=10,seed=None):\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    unique_simulation_combinations = df[[\"simulation_run\",\"is_accident_simulation\",\"accident_lane\"]].drop_duplicates().reset_index(drop=True)\n",
    "    unique_simulation_combinations = df[[\"simulation_run\",\"is_accident_simulation\",\"accident_lane\"]].drop_duplicates().reset_index(drop=True)\n",
    "    test_simulation_runs = unique_simulation_combinations.groupby([\"is_accident_simulation\",\"accident_lane\"]).simulation_run.unique().apply(lambda x: kfolds_from_array(x,k=k))\n",
    "    test_simulation_runs = test_simulation_runs.reset_index()\n",
    "    \n",
    "    for fold_number in range(k):\n",
    "        complete_test_index = []\n",
    "        for row in test_simulation_runs.itertuples():\n",
    "            current_test_index = (\n",
    "                (df.is_accident_simulation == row.is_accident_simulation)\n",
    "                &(df.accident_lane == row.accident_lane)\n",
    "                &(df.simulation_run.isin(row.simulation_run[fold_number]))\n",
    "            )\n",
    "            if len(complete_test_index):\n",
    "                complete_test_index = (complete_test_index | current_test_index)\n",
    "            else:\n",
    "                complete_test_index = current_test_index\n",
    "\n",
    "        train_index = ~complete_test_index\n",
    "        test_index = complete_test_index\n",
    "        #df_train = df[~complete_test_index].reset_index(drop=True)\n",
    "        #df_test = df[complete_test_index].reset_index(drop=True)\n",
    "        yield train_index, test_index #df_train, df_test\n",
    "\n",
    "def simulation_based_train_test_split(df, test_size=0.2, seed=None):\n",
    "    \"\"\"\n",
    "    Splits {df} into train and test datasets by their simulation-type with given {test_size}\n",
    "    \"\"\"\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    unique_simulation_combinations = (\n",
    "        df[[\"simulation_run\",\"is_accident_simulation\",\"accident_lane\"]].drop_duplicates().reset_index(drop=True)\n",
    "    )\n",
    "    test_simulation_runs = (\n",
    "        unique_simulation_combinations.groupby(\n",
    "            [\"is_accident_simulation\", \"accident_lane\"]\n",
    "        )\n",
    "        .simulation_run.unique()\n",
    "        .apply(lambda x: sample_from_array(x, freq=test_size))\n",
    "    )\n",
    "    test_simulation_runs = test_simulation_runs.reset_index()\n",
    "\n",
    "    complete_test_index = []\n",
    "    for row in test_simulation_runs.itertuples():\n",
    "        current_test_index = (\n",
    "            (df.is_accident_simulation == row.is_accident_simulation)\n",
    "            & (df.accident_lane == row.accident_lane)\n",
    "            & (df.simulation_run.isin(row.simulation_run))\n",
    "        )\n",
    "        if len(complete_test_index):\n",
    "            complete_test_index = complete_test_index | current_test_index\n",
    "        else:\n",
    "            complete_test_index = current_test_index\n",
    "    train_index = ~complete_test_index\n",
    "    test_index = complete_test_index\n",
    "    #df_train = df[~complete_test_index].reset_index(drop=True)\n",
    "    #df_test = df[complete_test_index].reset_index(drop=True)\n",
    "    return train_index, test_index #df_train, df_test\n",
    "\n",
    "def custom_cross_validation(models_list,performance_metrics_list,df_train,test_size=0.2,repetition_count=5,k_folds=10,seed=None):\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    results = []\n",
    "    for repetition in repetition_count:\n",
    "        df_train, df_validate = simulation_based_train_test_split(df=df_train,test_size=test_size)\n",
    "        x_train = df_train[feature_columns]\n",
    "        y_train = df_train[\"target\"]\n",
    "        x_validate = df_validate[feature_columns]\n",
    "        y_validate = df_validate[\"target\"]\n",
    "        for x,y in simulation_based_k_folds_split:\n",
    "            for model in models_list:\n",
    "                model.fit(x_train,y_train)\n",
    "                y_predicted = model.predict(x_validate)\n",
    "                for performance_metric in performance_metrics_list:\n",
    "                    performance_metric(y_validate,y_predicted)\n",
    "\n",
    "def custom_cross_validation(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    performance_metric,\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    repetition_count=5,\n",
    "    k_folds=10,\n",
    "    seed=None,\n",
    "    n_jobs=-1,\n",
    "    n_iter=10\n",
    "    ):\n",
    "    from sklearn.metrics import get_scorer\n",
    "\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    results = []\n",
    "    for repetition in range(repetition_count):\n",
    "        repetition_results = {\"repetition\": repetition}\n",
    "        train_index, validation_index = simulation_based_train_test_split(\n",
    "            df=df, test_size=test_size\n",
    "        )\n",
    "        df_train, df_validation = df[train_index], df[validation_index]\n",
    "        x_train = df_train[FEATURE_COLUMNS]\n",
    "        y_train = df_train[\"target\"]\n",
    "        x_validate = df_validation[FEATURE_COLUMNS]\n",
    "        y_validate = df_validation[\"target\"]\n",
    "        inner_cv = list(simulation_based_k_folds_split(df=df_train, k=k_folds))\n",
    "        #bayesian_optimizer = TuneSearchCV(\n",
    "        optimizer = RandomizedSearchCV(\n",
    "            estimator=pipe,\n",
    "            param_distributions=param_grid,\n",
    "            scoring=performance_metric,\n",
    "            n_jobs = n_jobs,\n",
    "            cv=inner_cv,\n",
    "            n_iter = n_iter,\n",
    "            verbose=2,\n",
    "            return_train_score=True,\n",
    "            #search_optimization=\"random\",#\"bayesian\",\n",
    "        )\n",
    "        cv_results = optimizer.fit(x_train, y_train)\n",
    "        #cv_results = optimizer.cv_results_\n",
    "        repetition_results[\"cv_results\"] = cv_results\n",
    "        best_model = optimizer.best_estimator_\n",
    "        validation_predictions = best_model.predict(x_validate)\n",
    "        scorer_function = get_scorer(performance_metric)._score_func\n",
    "        score = scorer_function(y_validate, validation_predictions)\n",
    "        repetition_results[\"best_model_validation_score\"] = score\n",
    "        results.append(repetition_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(ROOT_DIRECTORY,\"data/thesis_data/x_train_processed.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(ROOT_DIRECTORY,\"data/thesis_data/x_test_processed.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_feature_columns_path = os.path.join(ROOT_DIRECTORY,\"data/thesis_data/processed_feature_columns.txt\")\n",
    "with open(processed_feature_columns_path,\"r\") as reader:\n",
    "    FEATURE_COLUMNS = reader.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation: Hyper Parameter Tuning & Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_pipeline = Pipeline([\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"classifier\", RandomForestRegressor())\n",
    "]\n",
    ")\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        \"classifier\": [LinearRegression()],\n",
    "        \"classifier__n_jobs\": [-1],\n",
    "    },\n",
    "    {\n",
    "        \"classifier\": [Ridge()],\n",
    "        \"classifier__alpha\": uniform(0,10),#(0,100),\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        \"classifier\": [Lasso()],\n",
    "        \"classifier__alpha\": uniform(0,10),#(0,100),\n",
    "    },\n",
    "    {\n",
    "        \"classifier\": [BayesianRidge()],\n",
    "        \"classifier__alpha_1\": uniform(0,1),#(0,1),\n",
    "        \"classifier__alpha_2\": uniform(0,1),#(0,1),\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "CPU times: user 42.7 s, sys: 37.5 s, total: 1min 20s\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = custom_cross_validation(\n",
    "    pipe=tuning_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    performance_metric = \"neg_mean_squared_error\",\n",
    "    df=df_train,\n",
    "    test_size=0.2,\n",
    "    repetition_count=5,\n",
    "    k_folds=5,\n",
    "    seed=5,\n",
    "    n_jobs=50,\n",
    "    n_iter=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ROOT_DIRECTORY,\"data/results/cv_lm.pkl\"),\"wb\") as writer:\n",
    "    dill.dump(obj=results,file=writer,recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook cross_validation_LM.ipynb to python\n",
      "[NbConvertApp] Writing 10031 bytes to cross_validation_LM.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to python cross_validation_LM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = results[0][\"cv_results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = np.Inf\n",
    "best_model = None\n",
    "for result in results:\n",
    "    if result[\"best_model_validation_score\"] < best_result:\n",
    "        best_model = results[0][\"cv_results\"].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(x_train)\n",
    "x_test_scaled = min_max_scaler.transform(x_test)\n",
    "y_pred = best_model.predict(x_test_scaled)\n",
    "\n",
    "test_mse = mean_squared_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].keys()\n",
    "#results: list\n",
    "#results[0]: dict('repetition', 'cv_results', 'best_model_test_scores')\n",
    "#results[0][\"repetition\"]: int\n",
    "#results[0][\"cv_results\"]: sklearn.model_selection._search.RandomizedSearchCV\n",
    "#results[0][\"cv_results\"].best_params_\n",
    "#results[0][\"best_model_validation_score\"]: float"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
