{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sublime-deadline",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "matched-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from warnings import filterwarnings\n",
    "from pprint import pprint\n",
    "import gc\n",
    "filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-brazilian",
   "metadata": {},
   "source": [
    "## Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "resistant-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIRECTORY = \"/home/kaan.aytekin/Thesis\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-specification",
   "metadata": {},
   "source": [
    "## UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "concrete-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_array(array,freq):\n",
    "    array_size = len(array)\n",
    "    sample_size = int(np.ceil(array_size*freq))\n",
    "    array_slicer = np.zeros(array_size)\n",
    "    test_index =  np.random.choice(range(0,array_size),size=sample_size,replace=False)\n",
    "    array_slicer[test_index] = 1\n",
    "    return array[array_slicer.astype(bool)]\n",
    "\n",
    "\n",
    "def simulation_based_train_test_split(df, test_size=0.2, seed=None):\n",
    "    \"\"\"\n",
    "    Splits {df} into train and test datasets by their simulation-type with given {test_size}\n",
    "    \"\"\"\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    unique_simulation_combinations = (\n",
    "        df[non_feature_columns[:-4]].drop_duplicates().reset_index(drop=True)\n",
    "    )\n",
    "    test_simulation_runs = (\n",
    "        unique_simulation_combinations.groupby(\n",
    "            [\"is_accident_simulation\", \"accident_lane\"]\n",
    "        )\n",
    "        .simulation_run.unique()\n",
    "        .apply(lambda x: sample_from_array(x, freq=test_size))\n",
    "    )\n",
    "    test_simulation_runs = test_simulation_runs.reset_index()\n",
    "\n",
    "    complete_test_index = []\n",
    "    for row in test_simulation_runs.itertuples():\n",
    "        current_test_index = (\n",
    "            (df.is_accident_simulation == row.is_accident_simulation)\n",
    "            & (df.accident_lane == row.accident_lane)\n",
    "            & (df.simulation_run.isin(row.simulation_run))\n",
    "        )\n",
    "        if len(complete_test_index):\n",
    "            complete_test_index = complete_test_index | current_test_index\n",
    "        else:\n",
    "            complete_test_index = current_test_index\n",
    "    train_index = ~complete_test_index\n",
    "    test_index = complete_test_index\n",
    "    #df_train = df[~complete_test_index].reset_index(drop=True)\n",
    "    #df_test = df[complete_test_index].reset_index(drop=True)\n",
    "    return train_index, test_index #df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-responsibility",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "modular-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineered_data_path = os.path.join(ROOT_DIRECTORY,\"data/thesis_data/feature_engineered_data.csv\")\n",
    "df = pd.read_csv(feature_engineered_data_path)\n",
    "#df = df.replace(np.Inf,9999999)\n",
    "# Rephrasing the problem with another target?\n",
    "df[\"target_delay_time_diff\"] = df[\"target_delay_time\"] - df[\"delay_time_sec\"]\n",
    "df_columns = list(df.columns)\n",
    "# Non-feature columns\n",
    "non_feature_columns = [\"simulation_run\", \"is_accident_simulation\", \n",
    "                       \"accident_location\", \"accident_start_time\", \n",
    "                       \"accident_duration\", \"accident_lane\", \n",
    "                       \"prev_detector_detector_number\",\"next_detector_detector_number\",\n",
    "                       \"detector_number\", \"timestamp\"\n",
    "]\n",
    "target_columns = [\"target_delay_time\", \"target_delay_time_diff\"]\n",
    "feature_columns = [column for column in df_columns if column not in non_feature_columns + target_columns]\n",
    "feature_columns = [\"time_after_accident_started\", \"distance_to_accident\"] + [column for column in feature_columns if column not in [\"time_after_accident_started\", \"distance_to_accident\"]]\n",
    "# Reorder the data\n",
    "df = df[non_feature_columns + feature_columns + target_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "advanced-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_path = os.path.join(ROOT_DIRECTORY,f\"data/thesis_data/feature_names_list.txt\")\n",
    "with open(feature_list_path,\"w+\") as writer:\n",
    "    writer.write('\\n'.join(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-steal",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intelligent-seeker",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'simulation_based_train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2a99ceaa81bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulation_based_train_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnon_feature_columns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfeature_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'simulation_based_train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "train_index, test_index = simulation_based_train_test_split(df, test_size=0.2, seed=5)\n",
    "df_train = df[train_index]\n",
    "df_test = df[test_index]\n",
    "\n",
    "x_train = df_train[non_feature_columns + feature_columns]\n",
    "y_train = df_train[[\"target_delay_time\"]]\n",
    "\n",
    "x_test = df_test[non_feature_columns + feature_columns]\n",
    "y_test = df_test[[\"target_delay_time\"]]\n",
    "\n",
    "del df, df_train, df_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "second-lender",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((474806, 178), (124950, 178))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-speaker",
   "metadata": {},
   "source": [
    "## Serialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "heavy-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_to_serialize,df_name in zip([x_train,y_train,x_test,y_test],[\"x_train\",\"y_train\",\"x_test\",\"y_test\"]):\n",
    "    csv_path = os.path.join(ROOT_DIRECTORY,f\"data/thesis_data/{df_name}.csv\")\n",
    "    df_to_serialize.to_csv(csv_path,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
